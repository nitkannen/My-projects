{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM,Dense,Input,Embedding, Bidirectionalal, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lines = []\n",
    "target_lines = []\n",
    "for line in open('robert_frost.txt'):\n",
    "    line = (line.rstrip())\n",
    "    \n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    input_line = '<sos> '+ line\n",
    "    target_line = line + ' <eos>'\n",
    "    \n",
    "    input_lines.append(input_line)\n",
    "    target_lines.append(target_line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_line = []\n",
    "for line in open('robert_frost.txt'):\n",
    "    input_line.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = input_lines + target_lines  # for tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(all_lines)\n",
    "input_lines = tokenizer.texts_to_sequences(input_lines)\n",
    "target_lines = tokenizer.texts_to_sequences(target_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(input_linesa) = np.array(input_lines)\n",
    "target_linesa = np.array(target_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlength = max(len(s) for s in input_linesa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lines = pad_sequences(input_lines,maxlength, padding='post',\n",
    "    truncating='post')\n",
    "target_lines = pad_sequences(target_lines,maxlength, padding='post',\n",
    "    truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\senthilkumar\\Desktop\\ML\n"
     ]
    }
   ],
   "source": [
    "cd C:\\\\Users\\\\senthilkumar\\\\Desktop\\\\ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding vecs\n",
    "word2vec = {}\n",
    "with open('glove.6B.100d.txt',encoding =\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        vec = np.asarray(values[1:],dtype = 'float32')\n",
    "        word = values[0]\n",
    "        word2vec[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lines_onehot = to_categorical(target_lines,num_classes=vocab_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1436, 12, 3057)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lines_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3057)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target_lines_onehot[1,:,].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\senthilkumar\\\\Desktop\\\\ML'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the embedding matrix to be put into embedding layer\n",
    "embedding_matrix = np.zeros((vocab_size,100))\n",
    "for i,word in tokenizer.index_word.items():\n",
    "    vec = word2vec.get(word)\n",
    "    if vec  is not None:\n",
    "        embedding_matrix[i,:] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(vocab_size,100,weights = [embedding_matrix],trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\senthilkumar\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "input_ = Input(shape = (maxlength,))\n",
    "initial_h = Input(shape = (LATENT_DIM,))\n",
    "initial_c = Input(shape  = (LATENT_DIM,))\n",
    "x = embedding(input_)\n",
    "lstm_ = LSTM(LATENT_DIM,return_sequences=True,return_state=True)\n",
    "x,_,_ = lstm_(x,initial_state = [initial_h,initial_c])\n",
    "dense_ = Dense(vocab_size,activation = 'softmax')\n",
    "output = dense_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_,initial_h,initial_c],output)\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = Adam(lr = 0.01),metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros((len(input_lines),LATENT_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1292 samples, validate on 144 samples\n",
      "Epoch 1/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 4.3412 - acc: 0.3183 - val_loss: 5.7282 - val_acc: 0.2656\n",
      "Epoch 2/150\n",
      "1292/1292 [==============================] - 7s 6ms/step - loss: 4.3251 - acc: 0.3368 - val_loss: 5.7271 - val_acc: 0.2587\n",
      "Epoch 3/150\n",
      "1292/1292 [==============================] - 7s 6ms/step - loss: 4.3104 - acc: 0.3199 - val_loss: 5.7383 - val_acc: 0.2656\n",
      "Epoch 4/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 4.2933 - acc: 0.3409 - val_loss: 5.7656 - val_acc: 0.2685\n",
      "Epoch 5/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 4.2824 - acc: 0.3477 - val_loss: 5.7785 - val_acc: 0.2703\n",
      "Epoch 6/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 4.2667 - acc: 0.3595 - val_loss: 5.7937 - val_acc: 0.2662\n",
      "Epoch 7/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 4.2591 - acc: 0.3617 - val_loss: 5.7885 - val_acc: 0.2755\n",
      "Epoch 8/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 4.2467 - acc: 0.3745 - val_loss: 5.8104 - val_acc: 0.2697\n",
      "Epoch 9/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 4.2352 - acc: 0.3725 - val_loss: 5.8190 - val_acc: 0.2824\n",
      "Epoch 10/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 4.2215 - acc: 0.3889 - val_loss: 5.8223 - val_acc: 0.2836\n",
      "Epoch 11/150\n",
      "1292/1292 [==============================] - 10s 8ms/step - loss: 4.2100 - acc: 0.3951 - val_loss: 5.8603 - val_acc: 0.2789\n",
      "Epoch 12/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 4.2002 - acc: 0.3971 - val_loss: 5.8540 - val_acc: 0.2957\n",
      "Epoch 13/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 4.1866 - acc: 0.3968 - val_loss: 5.8521 - val_acc: 0.2940\n",
      "Epoch 14/150\n",
      "1292/1292 [==============================] - 10s 7ms/step - loss: 4.1721 - acc: 0.3962 - val_loss: 5.8723 - val_acc: 0.3067\n",
      "Epoch 15/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 4.1660 - acc: 0.3924 - val_loss: 5.8579 - val_acc: 0.2980\n",
      "Epoch 16/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 4.1490 - acc: 0.3965 - val_loss: 5.8793 - val_acc: 0.3056\n",
      "Epoch 17/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 4.1271 - acc: 0.4017 - val_loss: 5.8408 - val_acc: 0.3084\n",
      "Epoch 18/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 4.1117 - acc: 0.4034 - val_loss: 5.8786 - val_acc: 0.3137\n",
      "Epoch 19/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 4.0933 - acc: 0.4045 - val_loss: 5.8136 - val_acc: 0.3212\n",
      "Epoch 20/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 4.0742 - acc: 0.4051 - val_loss: 5.8435 - val_acc: 0.3194\n",
      "Epoch 21/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 4.0528 - acc: 0.4051 - val_loss: 5.8327 - val_acc: 0.3206\n",
      "Epoch 22/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 4.0289 - acc: 0.4057 - val_loss: 5.8240 - val_acc: 0.3206\n",
      "Epoch 23/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 4.0042 - acc: 0.4062 - val_loss: 5.7959 - val_acc: 0.3194\n",
      "Epoch 24/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 3.9816 - acc: 0.4057 - val_loss: 5.7871 - val_acc: 0.3166\n",
      "Epoch 25/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 3.9527 - acc: 0.4064 - val_loss: 5.7754 - val_acc: 0.3148\n",
      "Epoch 26/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 3.9200 - acc: 0.4064 - val_loss: 5.6808 - val_acc: 0.3160\n",
      "Epoch 27/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 3.8796 - acc: 0.4072 - val_loss: 5.6518 - val_acc: 0.3200\n",
      "Epoch 28/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 3.8457 - acc: 0.4076 - val_loss: 5.5910 - val_acc: 0.3241\n",
      "Epoch 29/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 3.8121 - acc: 0.4069 - val_loss: 5.5998 - val_acc: 0.3252\n",
      "Epoch 30/150\n",
      "1292/1292 [==============================] - 10s 7ms/step - loss: 3.7785 - acc: 0.4085 - val_loss: 5.5817 - val_acc: 0.3270\n",
      "Epoch 31/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.7453 - acc: 0.4112 - val_loss: 5.5863 - val_acc: 0.3218\n",
      "Epoch 32/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.7107 - acc: 0.4119 - val_loss: 5.5517 - val_acc: 0.3281\n",
      "Epoch 33/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.6754 - acc: 0.4127 - val_loss: 5.5576 - val_acc: 0.3275\n",
      "Epoch 34/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 3.6380 - acc: 0.4153 - val_loss: 5.5306 - val_acc: 0.3304\n",
      "Epoch 35/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 3.6025 - acc: 0.4153 - val_loss: 5.5551 - val_acc: 0.3287\n",
      "Epoch 36/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.5710 - acc: 0.4174 - val_loss: 5.5503 - val_acc: 0.3333\n",
      "Epoch 37/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.5281 - acc: 0.4228 - val_loss: 5.5461 - val_acc: 0.3345\n",
      "Epoch 38/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.4902 - acc: 0.4242 - val_loss: 5.5488 - val_acc: 0.3356\n",
      "Epoch 39/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 3.4551 - acc: 0.4308 - val_loss: 5.5267 - val_acc: 0.3356\n",
      "Epoch 40/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.4169 - acc: 0.4354 - val_loss: 5.5595 - val_acc: 0.3380\n",
      "Epoch 41/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 3.3826 - acc: 0.4381 - val_loss: 5.5311 - val_acc: 0.3380\n",
      "Epoch 42/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 3.3449 - acc: 0.4407 - val_loss: 5.5583 - val_acc: 0.3385\n",
      "Epoch 43/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 3.3113 - acc: 0.4419 - val_loss: 5.5373 - val_acc: 0.3380\n",
      "Epoch 44/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 3.2748 - acc: 0.4440 - val_loss: 5.5586 - val_acc: 0.3380\n",
      "Epoch 45/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.2408 - acc: 0.4458 - val_loss: 5.5570 - val_acc: 0.3362\n",
      "Epoch 46/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 3.2091 - acc: 0.4495 - val_loss: 5.5653 - val_acc: 0.3385\n",
      "Epoch 47/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.1764 - acc: 0.4505 - val_loss: 5.5593 - val_acc: 0.3426\n",
      "Epoch 48/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.1442 - acc: 0.4521 - val_loss: 5.5849 - val_acc: 0.3403\n",
      "Epoch 49/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.1110 - acc: 0.4577 - val_loss: 5.5658 - val_acc: 0.3397\n",
      "Epoch 50/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.0793 - acc: 0.4592 - val_loss: 5.5785 - val_acc: 0.3426\n",
      "Epoch 51/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 3.0508 - acc: 0.4600 - val_loss: 5.6095 - val_acc: 0.3414\n",
      "Epoch 52/150\n",
      "1292/1292 [==============================] - 8s 7ms/step - loss: 3.0193 - acc: 0.4648 - val_loss: 5.6018 - val_acc: 0.3426\n",
      "Epoch 53/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 2.9926 - acc: 0.4668 - val_loss: 5.6360 - val_acc: 0.3420\n",
      "Epoch 54/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 2.9658 - acc: 0.4705 - val_loss: 5.6471 - val_acc: 0.3432\n",
      "Epoch 55/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.9395 - acc: 0.4720 - val_loss: 5.6439 - val_acc: 0.3438\n",
      "Epoch 56/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.9133 - acc: 0.4753 - val_loss: 5.6610 - val_acc: 0.3443\n",
      "Epoch 57/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.8879 - acc: 0.4788 - val_loss: 5.6767 - val_acc: 0.3438\n",
      "Epoch 58/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.8694 - acc: 0.4790 - val_loss: 5.6770 - val_acc: 0.3443\n",
      "Epoch 59/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.8439 - acc: 0.4848 - val_loss: 5.6930 - val_acc: 0.3455\n",
      "Epoch 60/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.8170 - acc: 0.4856 - val_loss: 5.6946 - val_acc: 0.3443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/150\n",
      "1292/1292 [==============================] - 10s 7ms/step - loss: 2.7978 - acc: 0.4891 - val_loss: 5.7116 - val_acc: 0.3472\n",
      "Epoch 62/150\n",
      "1292/1292 [==============================] - 10s 8ms/step - loss: 2.7762 - acc: 0.4905 - val_loss: 5.7299 - val_acc: 0.3443\n",
      "Epoch 63/150\n",
      "1292/1292 [==============================] - 10s 7ms/step - loss: 2.7536 - acc: 0.4919 - val_loss: 5.7404 - val_acc: 0.3438\n",
      "Epoch 64/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.7301 - acc: 0.4951 - val_loss: 5.7313 - val_acc: 0.3461\n",
      "Epoch 65/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.7115 - acc: 0.4983 - val_loss: 5.7772 - val_acc: 0.3426\n",
      "Epoch 66/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 2.6883 - acc: 0.4997 - val_loss: 5.7413 - val_acc: 0.3461\n",
      "Epoch 67/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 2.6696 - acc: 0.5052 - val_loss: 5.7992 - val_acc: 0.3461\n",
      "Epoch 68/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 2.6484 - acc: 0.5020 - val_loss: 5.7839 - val_acc: 0.3472\n",
      "Epoch 69/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 2.6259 - acc: 0.5070 - val_loss: 5.8205 - val_acc: 0.3455\n",
      "Epoch 70/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.6085 - acc: 0.5077 - val_loss: 5.8074 - val_acc: 0.3484\n",
      "Epoch 71/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 2.5903 - acc: 0.5107 - val_loss: 5.8274 - val_acc: 0.3484\n",
      "Epoch 72/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.5727 - acc: 0.5139 - val_loss: 5.8622 - val_acc: 0.3455\n",
      "Epoch 73/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.5560 - acc: 0.5159 - val_loss: 5.8411 - val_acc: 0.3484\n",
      "Epoch 74/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.5385 - acc: 0.5178 - val_loss: 5.8621 - val_acc: 0.3490\n",
      "Epoch 75/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.5215 - acc: 0.5190 - val_loss: 5.8581 - val_acc: 0.3490\n",
      "Epoch 76/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.5061 - acc: 0.5210 - val_loss: 5.8531 - val_acc: 0.3449\n",
      "Epoch 77/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.4896 - acc: 0.5227 - val_loss: 5.8852 - val_acc: 0.3490\n",
      "Epoch 78/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.4763 - acc: 0.5246 - val_loss: 5.8709 - val_acc: 0.3484\n",
      "Epoch 79/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.4635 - acc: 0.5278 - val_loss: 5.8834 - val_acc: 0.3478\n",
      "Epoch 80/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.4478 - acc: 0.5290 - val_loss: 5.9056 - val_acc: 0.3490\n",
      "Epoch 81/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.4311 - acc: 0.5301 - val_loss: 5.9061 - val_acc: 0.3466\n",
      "Epoch 82/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.4157 - acc: 0.5340 - val_loss: 5.9193 - val_acc: 0.3461\n",
      "Epoch 83/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.4002 - acc: 0.5372 - val_loss: 5.9203 - val_acc: 0.3472\n",
      "Epoch 84/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 2.3860 - acc: 0.5390 - val_loss: 5.9401 - val_acc: 0.3478\n",
      "Epoch 85/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.3709 - acc: 0.5403 - val_loss: 5.9554 - val_acc: 0.3461\n",
      "Epoch 86/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 2.3605 - acc: 0.5402 - val_loss: 5.9785 - val_acc: 0.3461\n",
      "Epoch 87/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 2.3490 - acc: 0.5433 - val_loss: 5.9671 - val_acc: 0.3484\n",
      "Epoch 88/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 2.3320 - acc: 0.5435 - val_loss: 5.9835 - val_acc: 0.3495\n",
      "Epoch 89/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 2.3184 - acc: 0.5478 - val_loss: 5.9893 - val_acc: 0.3478\n",
      "Epoch 90/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 2.3086 - acc: 0.5484 - val_loss: 6.0154 - val_acc: 0.3495\n",
      "Epoch 91/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 2.2948 - acc: 0.5489 - val_loss: 6.0371 - val_acc: 0.3455\n",
      "Epoch 92/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 2.2820 - acc: 0.5520 - val_loss: 6.0442 - val_acc: 0.3472\n",
      "Epoch 93/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 2.2707 - acc: 0.5535 - val_loss: 6.0463 - val_acc: 0.3461\n",
      "Epoch 94/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 2.2558 - acc: 0.5538 - val_loss: 6.0433 - val_acc: 0.3472\n",
      "Epoch 95/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.2459 - acc: 0.5548 - val_loss: 6.0649 - val_acc: 0.3455\n",
      "Epoch 96/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 2.2337 - acc: 0.5573 - val_loss: 6.0619 - val_acc: 0.3472\n",
      "Epoch 97/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.2249 - acc: 0.5554 - val_loss: 6.0863 - val_acc: 0.3461\n",
      "Epoch 98/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.2133 - acc: 0.5593 - val_loss: 6.0978 - val_acc: 0.3455\n",
      "Epoch 99/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.2006 - acc: 0.5614 - val_loss: 6.0872 - val_acc: 0.3472\n",
      "Epoch 100/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.1920 - acc: 0.5646 - val_loss: 6.1170 - val_acc: 0.3466\n",
      "Epoch 101/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.1836 - acc: 0.5658 - val_loss: 6.1245 - val_acc: 0.3426\n",
      "Epoch 102/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.1726 - acc: 0.5637 - val_loss: 6.1192 - val_acc: 0.3466\n",
      "Epoch 103/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.1624 - acc: 0.5668 - val_loss: 6.1228 - val_acc: 0.3461\n",
      "Epoch 104/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 2.1494 - acc: 0.5695 - val_loss: 6.1508 - val_acc: 0.3466\n",
      "Epoch 105/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.1417 - acc: 0.5693 - val_loss: 6.1723 - val_acc: 0.3443\n",
      "Epoch 106/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.1364 - acc: 0.5684 - val_loss: 6.1729 - val_acc: 0.3426\n",
      "Epoch 107/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.1237 - acc: 0.5711 - val_loss: 6.1390 - val_acc: 0.3461\n",
      "Epoch 108/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.1147 - acc: 0.5748 - val_loss: 6.1842 - val_acc: 0.3461\n",
      "Epoch 109/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.1040 - acc: 0.5760 - val_loss: 6.1645 - val_acc: 0.3461\n",
      "Epoch 110/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.0971 - acc: 0.5748 - val_loss: 6.1840 - val_acc: 0.3484\n",
      "Epoch 111/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.0907 - acc: 0.5766 - val_loss: 6.2164 - val_acc: 0.3472\n",
      "Epoch 112/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.0797 - acc: 0.5772 - val_loss: 6.2140 - val_acc: 0.3461\n",
      "Epoch 113/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.0737 - acc: 0.5778 - val_loss: 6.2184 - val_acc: 0.3472\n",
      "Epoch 114/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.0616 - acc: 0.5818 - val_loss: 6.2306 - val_acc: 0.3449\n",
      "Epoch 115/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.0539 - acc: 0.5808 - val_loss: 6.2257 - val_acc: 0.3455\n",
      "Epoch 116/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 2.0473 - acc: 0.5817 - val_loss: 6.2462 - val_acc: 0.3455\n",
      "Epoch 117/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.0383 - acc: 0.5844 - val_loss: 6.2545 - val_acc: 0.3484\n",
      "Epoch 118/150\n",
      "1292/1292 [==============================] - 7s 6ms/step - loss: 2.0296 - acc: 0.5840 - val_loss: 6.2333 - val_acc: 0.3472\n",
      "Epoch 119/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 2.0222 - acc: 0.5849 - val_loss: 6.2997 - val_acc: 0.3420\n",
      "Epoch 120/150\n",
      "1292/1292 [==============================] - 8s 7ms/step - loss: 2.0129 - acc: 0.5875 - val_loss: 6.2518 - val_acc: 0.3466\n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1292/1292 [==============================] - 7s 6ms/step - loss: 2.0032 - acc: 0.5886 - val_loss: 6.2698 - val_acc: 0.3466\n",
      "Epoch 122/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 1.9978 - acc: 0.5891 - val_loss: 6.3172 - val_acc: 0.3438\n",
      "Epoch 123/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 1.9898 - acc: 0.5918 - val_loss: 6.3017 - val_acc: 0.3438\n",
      "Epoch 124/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.9809 - acc: 0.5949 - val_loss: 6.3136 - val_acc: 0.3455\n",
      "Epoch 125/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.9824 - acc: 0.5942 - val_loss: 6.3353 - val_acc: 0.3443\n",
      "Epoch 126/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.9759 - acc: 0.5929 - val_loss: 6.3019 - val_acc: 0.3484\n",
      "Epoch 127/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 1.9715 - acc: 0.5938 - val_loss: 6.3127 - val_acc: 0.3466\n",
      "Epoch 128/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 1.9599 - acc: 0.5964 - val_loss: 6.3382 - val_acc: 0.3466\n",
      "Epoch 129/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 1.9521 - acc: 0.5965 - val_loss: 6.3310 - val_acc: 0.3455\n",
      "Epoch 130/150\n",
      "1292/1292 [==============================] - 7s 6ms/step - loss: 1.9492 - acc: 0.5975 - val_loss: 6.3362 - val_acc: 0.3455\n",
      "Epoch 131/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.9445 - acc: 0.5988 - val_loss: 6.3410 - val_acc: 0.3461\n",
      "Epoch 132/150\n",
      "1292/1292 [==============================] - 7s 6ms/step - loss: 1.9306 - acc: 0.6030 - val_loss: 6.3685 - val_acc: 0.3455\n",
      "Epoch 133/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.9207 - acc: 0.6058 - val_loss: 6.3660 - val_acc: 0.3455\n",
      "Epoch 134/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.9223 - acc: 0.6013 - val_loss: 6.3835 - val_acc: 0.3455\n",
      "Epoch 135/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.9141 - acc: 0.6049 - val_loss: 6.3728 - val_acc: 0.3466\n",
      "Epoch 136/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.9048 - acc: 0.6042 - val_loss: 6.3692 - val_acc: 0.3461\n",
      "Epoch 137/150\n",
      "1292/1292 [==============================] - 7s 6ms/step - loss: 1.8946 - acc: 0.6082 - val_loss: 6.4012 - val_acc: 0.3455\n",
      "Epoch 138/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.8852 - acc: 0.6092 - val_loss: 6.4035 - val_acc: 0.3449\n",
      "Epoch 139/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 1.8817 - acc: 0.6098 - val_loss: 6.4145 - val_acc: 0.3472\n",
      "Epoch 140/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 1.8739 - acc: 0.6096 - val_loss: 6.4370 - val_acc: 0.3449\n",
      "Epoch 141/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 1.8701 - acc: 0.6104 - val_loss: 6.4399 - val_acc: 0.3449\n",
      "Epoch 142/150\n",
      "1292/1292 [==============================] - 6s 5ms/step - loss: 1.8674 - acc: 0.6100 - val_loss: 6.4303 - val_acc: 0.3466\n",
      "Epoch 143/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 1.8604 - acc: 0.6102 - val_loss: 6.4317 - val_acc: 0.3478\n",
      "Epoch 144/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.8545 - acc: 0.6124 - val_loss: 6.4481 - val_acc: 0.3443\n",
      "Epoch 145/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.8479 - acc: 0.6135 - val_loss: 6.4514 - val_acc: 0.3449\n",
      "Epoch 146/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 1.8415 - acc: 0.6158 - val_loss: 6.4697 - val_acc: 0.3478\n",
      "Epoch 147/150\n",
      "1292/1292 [==============================] - 8s 6ms/step - loss: 1.8329 - acc: 0.6193 - val_loss: 6.4491 - val_acc: 0.3461\n",
      "Epoch 148/150\n",
      "1292/1292 [==============================] - 9s 7ms/step - loss: 1.8310 - acc: 0.6167 - val_loss: 6.4747 - val_acc: 0.3501\n",
      "Epoch 149/150\n",
      "1292/1292 [==============================] - 7s 5ms/step - loss: 1.8232 - acc: 0.6186 - val_loss: 6.4761 - val_acc: 0.3478\n",
      "Epoch 150/150\n",
      "1292/1292 [==============================] - 7s 6ms/step - loss: 1.8211 - acc: 0.6212 - val_loss: 6.4842 - val_acc: 0.3472\n"
     ]
    }
   ],
   "source": [
    "z = np.zeros((len(input_lines),LATENT_DIM))\n",
    "r= model.fit([input_lines,z,z],target_lines_onehot,batch_size = 128,epochs = 150,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "    input_generate = Input(shape  = (1,))\n",
    "    x = embedding(input_generate)\n",
    "    x,h,c = lstm_(x,initial_state = [initial_h,initial_c])\n",
    "    output_ = dense_(x)\n",
    "    sampling_model = Model([input_generate,initial_h,initial_c],[output_,h,c])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = tokenizer.index_word\n",
    "word_to_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he has one, had sure in snow and time'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences():\n",
    "    \n",
    "    inp = np.array([[ word_to_idx['<sos>'] ]])\n",
    "    h = np.zeros((1,LATENT_DIM))\n",
    "    c = np.zeros((1,LATENT_DIM))\n",
    "    eos = word_to_idx['<eos>']\n",
    "    output_sentence = []\n",
    "    for _ in range(maxlength):\n",
    "        if ((inp[0,0] == eos) and(len(output_sentence)<=3)):\n",
    "            inp[0,0] = previous_word\n",
    "        o,h,c = sampling_model.predict([inp,h,c])\n",
    "        probs = o[0,0]\n",
    "        #if np.argmax(probs) == 0:\n",
    "            #print('lol')\n",
    "        probs[0] = 0\n",
    "        probs /= sum(probs)\n",
    "        word_index= np.random.choice(len(probs),p = probs)\n",
    "        if len(output_sentence)>3:\n",
    "            if eos == word_index:\n",
    "                break\n",
    "        if(eos == word_index):\n",
    "            continue\n",
    "        #word = idx_to_word.get(word_index)\n",
    "        output_sentence.append(idx_to_word.get(word_index, '<WTF %s>' % word_index))\n",
    "        previous_word = inp[0,0]\n",
    "        inp[0,0] = word_index\n",
    "    return ' '.join(output_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parawise(paras):\n",
    "    for _ in range(6):\n",
    "        print('\\n')\n",
    "        for _ in range(paras):\n",
    "            sent = (generate_sentences())\n",
    "            print(sent)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "it sailing up one of fountain a certain coolness came hudson's double\n",
      "and technical.- and try\n",
      "with having eased her sills they're with poems.\n",
      "of one that is our wildest ears suggest it? of spray;\n",
      "\n",
      "\n",
      "once one street,\n",
      "it's too, son. no more\n",
      "one that surer struck en masse\n",
      "'i'm aftermath,\n",
      "\n",
      "\n",
      "at kiting font bed\n",
      "two roads diverged in on may, in the meeting say go think\n",
      "and now left's do what will we know in earnest.\n",
      "they weren't;\n",
      "\n",
      "\n",
      "'i want what the ominous wood, if and stars,\n",
      "half closes the buttons hillside your horse made him do\n",
      "'you have ideas yet empty.\n",
      "if a (now make me to make you _can_ one then going\n",
      "\n",
      "\n",
      "makes on position they leave such a guide.'\n",
      "like lightning or one, who wrote the birds\n",
      "'the height of heart they came with being are keeping the feet\n",
      "i thought of me a ones though,\n",
      "\n",
      "\n",
      "a shattered statement of relationship,\n",
      "a book of was roar\n",
      "and frozen horse cellar hole,\n",
      "'i must be mad.'\n"
     ]
    }
   ],
   "source": [
    "generate_parawise(4) ### This is the final generated poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
