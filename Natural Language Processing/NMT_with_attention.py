# -*- coding: utf-8 -*-
"""NMT_with_attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T9ik-Fag3fLlBOc1KBDqq-9ucva9isqJ
"""

import pandas as pd
import numpy as np
import re

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

!pip install tensorflow==2.2.0-rc0

import codecs

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import LSTM,Dense,Embedding,Input,Bidirectional, Concatenate
from keras.layers import RepeatVector, Dot, Permute, Lambda, Layer, Dropout, Conv1D
from keras.models import Model
from keras.callbacks import TensorBoard, LearningRateScheduler, ModelCheckpoint
from keras.callbacks import EarlyStopping,ReduceLROnPlateau, LambdaCallback
from keras.utils import  plot_model
from keras.utils import to_categorical
from keras import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import keras.backend as K
import tensorflow as tf
#if len(K.tensorflow_backend._get_available_gpus()) > 0:
# from keras.layers import CuDNNLSTM as LSTM
# from keras.layers import CuDNNGRU as GRU

"""# Data Exploration"""

ls

cd C:\Users\Nithish\Desktop\ML\Datasets\hin-eng\en-hi.txt (1)

cd "en-hi.txt (1)"

input_text = []
with open('OpenSubtitles.en-hi.en',encoding = 'utf8') as f:
    for line in f:
        input_data = line.rstrip().lower()
        input_text.append(input_data)

len(input_text)

df = pd.DataFrame(target_data_input)

df.columns = ['sentences']

df['length'] = df['sentences'].apply(lambda x: len(x.split()))

plt.figure(figsize = (20,20))
plt.hist(df['length'],bins = 200)

df['length']

"That's a different category.\n".lower()

input_text[3546]

df = pd.DataFrame(target_data_input)

df.columns = ['Sentences']

plt.figure(figsize = (20,10))
plt.hist(df['Sentences'].apply(lambda x:len(x.split())), bins = 100)

# Chosen size for English corpus is 15

df['Sentences'].apply(lambda x:len(x.split())).mean() # mean english sentence size = 5.47

target_text = []
with open('OpenSubtitles.en-hi.hi',encoding = 'utf8') as f:
    for line in f:
        target_data = line.rstrip()
        target_text.append(target_data)

len(target_text)

target_text[3546]

df = pd.DataFrame(target_text2)

df.columns = ['Sentences']

plt.figure(figsize = (10,6))
plt.hist(df['Sentences'].apply(lambda x:len(x.split())), bins = 100)

#Chosen size for Hindi corpus at 18

df['Sentences'].apply(lambda x:len(x.split())).mean() # Mean Hindi sentence size = 6.24

"""Dataset2"""

pwd

cd C:\\Users\\Nithish\\Desktop\\ML\\Datasets\\hin-eng

with open('hin.txt',encoding = 'utf8') as f:
    for line in f:
        
        input_data = line.rstrip().lower().split('\t')[0]
        target_data = line.rstrip().split('\t')[1] 
        input_text.append(input_data)
        target_text.append(target_data)

input_text[95733]

len(target_text)

input_text[95711]

target_text[95711]

df_input = pd.DataFrame(input_data)

df_input.columns = ['Sentences']

plt.figure(figsize = (10,6))
plt.hist((df_input["Sentences"].apply(lambda x:len(x.split()))),bins = 25)
plt.title('Length distribution of sentences')

#considered examples 
plt.figure(figsize = (10,6))
plt.hist((df_input["Sentences"].iloc[:2150].apply(lambda x:len(x.split()))),bins = 17)
plt.title('Length distribution of sentences')

type(f)

len(input_data[2150].split())



df_target = pd.DataFrame(target_data)

df_target.columns = ["sentences"]

df_target

plt.figure(figsize = (10,6))
plt.hist((df_target["sentences"].apply(lambda x:len(x.split()))),bins = 30)
plt.title('Length distribution of sentences')

#considered examples
plt.figure(figsize = (10,6))
plt.hist((df_target["sentences"].iloc[:2150].apply(lambda x:len(x.split()))),bins = 27)
plt.title('Length distribution of sentences')



tokens = Tokenizer(filters = '#$%&()*@[\\]^_`{|}~<=>\n')

tokens.fit_on_texts((input_text))
input_data_token = tokens.texts_to_sequences(input_text)

len(tokens.index_word)                # approximately 41443 unique words used in the english data

tokens_hin = Tokenizer(filters = '#$%&()*@[\\]^_`{|}~<=>\n')

tokens_hin.fit_on_texts(target_text)
target_data_token = tokens.texts_to_sequences(target_text)

len(tokens_hin.index_word)            #approximately 41643 unique words in the hindi data set

tester = []
with open('testing_english.txt',encoding = 'utf8') as f:
    for line in f:
        input_text = line.rstrip().lower()
        tester.append(input_text)

df = pd.DataFrame(tester)

df.columns = ['Sentences']
df['length'] = df['Sentences'].apply(lambda x:len(x.split()))
plt.figure(figsize = (10,6))
plt.hist(df['length'], bins = 30)

"""# Load Text"""



from google.colab import drive
drive.mount('/content/drive', force_remount= True)

ls

cd drive/

cd My\ Drive

def filter_english_in_hindi(line):
    string = line.rstrip().lower().split('\t')[0].split()
        #if i == 304:
            #print(string)
    new_string_words = []
    for word in string:
        if not re.search(r'[a-zA-Z]', word):
            new_string_words.append(word)
    filtered_string = ' '.join(new_string_words)
    filtered_string = filtered_string.replace('( ) ',' ')
    
    return filtered_string

target_data_input = []
target_data_output = []
input_data = []
with open('OpenSubtitles.en-hi.en',encoding = 'utf8') as f:
    for line in f:
        input_text = line.rstrip().lower()
        input_data.append(input_text)
k = 0
index_list = []
for i, j in enumerate(input_data):
    if len(j.split()) > 4 and len(j.split())< 11:
        k +=1
        index_list.append(i)
index_list = np.array(index_list)
input_data = np.array(input_data)[index_list]
input_data = input_data.tolist()

with open('OpenSubtitles.en-hi.hi',encoding = 'utf8') as f:
    for line in f:
        target_text = line.rstrip()
        target_text = filter_english_in_hindi(target_text)
        target_input = '<sos> ' +  target_text
        target_output = target_text + ' <eos>'
        target_data_input.append(target_input)
        target_data_output.append(target_output)
target_data_output = np.array(target_data_output)[index_list]
target_data_input = np.array(target_data_input)[index_list]
target_data_output = target_data_output.tolist()
target_data_input = target_data_input.tolist()

#sent_list =[]
def checkout_text_samples():
    with open('Hindi',encoding = 'utf8') as f:
        for i,line in enumerate(f):
            #sent_list.append(line.rstrip().lower().split('\t')[0])
            string = line.rstrip().lower().split('\t')[0].split()
            #if i == 304:
                #print(string)
            new_string_words = []
            for word in string:
                if not re.search(r'[a-zA-Z]', word):
                    new_string_words.append(word)
            new_string = ' '.join(new_string_words)
            new_string = new_string.replace('( ) ',' ')
            #if i == 304:
                #print(new_string)
            print("Training sample {}:{} ".format(i, new_string))

with open('hin.txt',encoding = 'utf8') as f:
    for line in f:
        
        input_text = line.rstrip().lower().split('\t')[0]
        target_text = line.rstrip().split('\t')[1] 
        target_text = filter_english_in_hindi(target_text)
        target_input = '<sos> ' +  target_text
        target_output = target_text + ' <eos>'
        input_data.append(input_text)
        target_data_input.append(target_input)
        target_data_output.append(target_output)

len(target_data_input)

len(input_data)

target_data_input_ = []
target_data_output_ = []
input_data_ = []
with open('traing_english.txt',encoding = 'utf8') as f:
    for line in f:
        input_text = line.rstrip().lower()
        input_data_.append(input_text)
        
k = 0
index_list = []
for i, j in enumerate(input_data_):
    if len(j.split()) > 3 and len(j.split())< 16:
        k +=1
        index_list.append(i)
index_list = np.array(index_list)
input_data_ = np.array(input_data_)[index_list]
input_data_ = input_data_.tolist()

with open('traing_hindi.txt',encoding = 'utf8') as f:
    for line in f:
        target_text = line.rstrip()
        target_text = filter_english_in_hindi(target_text)
        target_input = '<sos> ' +  target_text
        target_output = target_text + ' <eos>'
        target_data_input_.append(target_input)
        target_data_output_.append(target_output)
target_data_output_ = np.array(target_data_output_)[index_list]
target_data_input_ = np.array(target_data_input_)[index_list]
target_data_output_ = target_data_output_.tolist()
target_data_input_ = target_data_input_.tolist()



test_input_data = []
test_target_output = []
test_target_input = []
with open('testing_english.txt',encoding = 'utf8') as f:
    for line in f:
        input_text = line.rstrip().lower()
        test_input_data.append(input_text)
 ### correct code errors
k = 0
index_list = []
for i, j in enumerate(test_input_data):
    if len(j.split()) > 3 and len(j.split())< 16:
        k +=1
        index_list.append(i)
index_list = np.array(index_list)
test_input_data = (np.array(test_input_data)[index_list]).tolist()

len(test_input_data)

with open('testing_hindi.txt',encoding = 'utf8') as f:
    for line in f:
        target_text = line.rstrip()
        target_text = filter_english_in_hindi(target_text)
        target_input = '<sos> ' +  target_text
        target_output = target_text + ' <eos>'
        test_target_input.append(target_input)
        test_target_output.append(target_output)
test_target_output = np.array(test_target_output)[index_list].tolist()
test_target_input = np.array(test_target_input)[index_list].tolist()
#target_data_output_ = target_data_output_.tolist()
#target_data_input_ = target_data_input_.tolist()

len(test_target_input)

input_data = input_data + input_data_ #+ test_input_data

target_data_input = target_data_input + target_data_input_ #+ test_target_input

target_data_output = target_data_output + target_data_output_ #+ test_target_output

with open('English',encoding = 'utf8') as f:
    for line in f:
        input_text = line.rstrip().lower()
        input_data.append(input_text)

with open('Hindi',encoding = 'utf8') as f:
    for line in f:
        target_text = line.rstrip()
        target_text = filter_english_in_hindi(target_text)
        target_input = '<sos> ' +  target_text
        target_output = target_text + ' <eos>'
        target_data_input.append(target_input)
        target_data_output.append(target_output)

filter_english_in_hindi('Sociables, finger rolls... ...और 7 प्रकार के जो तेरे पास हैं.')

len(input_data)

len(target_data_input)

len(target_data_output)

"""## Text Preprocessing

Tokenizing
"""

tokenizer_input = Tokenizer(filters = '#$%&()*@[\\]^_`,\'\"-{|}~<=>.!+/:;?....')

tokenizer_input.fit_on_texts(input_data)
input_tokens= tokenizer_input.texts_to_sequences(input_data)

len(tokenizer_input.index_word)

#tokenizer_input.index_word

all_target = target_data_input + target_data_output

tokenizer_output = Tokenizer(filters = '#$%&()*@[\\]^_`,-\'\"\u200d{|}~<=>.+/!:;?....।')

tokenizer_output.fit_on_texts(all_target)

target_tokens_input = tokenizer_output.texts_to_sequences(target_data_input)
target_tokens_output = tokenizer_output.texts_to_sequences(target_data_output)

len(tokenizer_output.index_word)

#tokenizer_output.index_word

HINDI_VOCAB_SIZE = len(tokenizer_output.word_index) + 1
ENGLISH_VOCAB_SIZE = len(tokenizer_input.word_index) + 1
MAX_HIN_LEN = 20
MAX_ENG_LEN = 20

"""Padding"""

input_tokens_pad = pad_sequences(input_tokens,maxlen = 20,padding = 'post')
#input_tokens_pad[2148]

input_tokens_pad.shape

target_tokens_input_pad = pad_sequences(target_tokens_input,maxlen = 20,padding = 'post')
#target_tokens_input_pad[21099]
target_tokens_output_pad = pad_sequences(target_tokens_output,maxlen = 20,padding = 'post')
#target_tokens_output_pad[21009]

target_tokens_output_pad.shape

target_tokens_input_pad.shape

#target_tokens_output_one_hot = to_categorical(target_tokens_output_pad[:10000])

#Dont run yet

decoder_targets = np.zeros(
  (
    1000,
    20,
    42000
  ),
  dtype='float32'
)

# assign the values
for i, d in enumerate(target_tokens_output_pad[:1000]):
  for t, word in enumerate(d):
    if word > 0:
      decoder_targets[i, t, word] = 1

decoder_targets.shape

target_tokens_output_pad = []

target_tokens_output_one_hot = []

"""## Load Word **Vectors**"""

word2vec = {}
with open('glove.6B.300d.txt',encoding = 'utf-8') as f:
    for line in f:
        val = line.split()
        word = val[0]
        vec = np.asarray(val[1:], dtype = 'float32')
        word2vec[word] = vec

hindi_word2vec = {}
with open('cc.hi.300.vec',encoding = 'utf-8') as f:
    for line in f:
        val = line.split()
        word = val[0]
        vec = np.asarray(val[1:],dtype = 'float32')
        hindi_word2vec[word] = vec
        #word = line[0]
        #if word is not None:
            #vec = line[1:]
        #hindi_word2vec[word] = vec

english_embedding_matrix = np.zeros((ENGLISH_VOCAB_SIZE,300))
for i,j in tokenizer_input.word_index.items():
  #if j < ENGLISH_VOCAB_SIZE:
    vec = word2vec.get(i)
    if vec is not None:
        english_embedding_matrix[j,:] = vec

english_embedding_matrix.shape

hindi_embedding_matrix = np.zeros((HINDI_VOCAB_SIZE,300))
for i,j in tokenizer_output.word_index.items():
  #if j < HINDI_VOCAB_SIZE:
    vec = hindi_word2vec.get(i)
    if vec is not None:
        hindi_embedding_matrix[j,:] = vec

hindi_embedding_matrix.shape

"""# Seq2Seq Model"""

#Seq2Seq Model

encoder_embedding = Embedding(ENGLISH_VOCAB_SIZE,300, weights = [english_embedding_matrix])

encoder_input = Input(shape = (20,))
encoder_lstm = Bidirectional(LSTM(200, return_state =True))

x = encoder_embedding(encoder_input)
encoder_output,h1,c1,h2,c2 = encoder_lstm(x)
state_h = Concatenate(axis = 1)([h1,h2])
state_c = Concatenate(axis = 1)([c1,c2])
encoder_states = [state_h,state_c]

decoder_embedding = Embedding(HINDI_VOCAB_SIZE,300, weights = [hindi_embedding_matrix])

decoder_input = Input(shape = (20,))
decoder_lstm = LSTM(400, return_sequences=True,return_state=True)
decoder_dense1 = Dense(400,activation = 'relu')
decoder_dense2 = Dense(HINDI_VOCAB_SIZE,activation = 'softmax')

y = decoder_embedding(decoder_input)
decoder_output, _, _ = decoder_lstm(y, initial_state = encoder_states)
preoutput = decoder_dense1(decoder_output)
output = decoder_dense2(preoutput)

train_model = Model([encoder_input,decoder_input],output)

train_model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

train_model.summary()

plot_model(train_model, show_layer_names = False)

target_tokens_output_pad_ = target_tokens_output_pad.reshape((*target_tokens_output_pad.shape),1)

input_tokens_pad.shape

r = train_model.fit([input_tokens_pad,target_tokens_input_pad],target_tokens_output_pad_, batch_size = 256,epochs = 10,validation_split= 0.1)#, callbacks= [tb, early_stopper, lr_optimizer])

train_model.optimizer.lr





"""## Attention Model"""

## Encoder

encoder_input = Input(shape = (MAX_ENG_LEN,))
encoder_embedding = Embedding(ENGLISH_VOCAB_SIZE,300, weights = [english_embedding_matrix])
encoder_LSTM = Bidirectional(LSTM(200, return_sequences= True, return_state= False))
#encoder_CNN = Conv1D(256, 3, padding = 'same', activation = 'relu')

embedded_input = encoder_embedding(encoder_input)  # Shape = N X Tx X 300
#encoder_features = encoder_CNN(embedded_input)
encoder_output = encoder_LSTM(embedded_input) # shape = N X Tx X 400

## Decoder

decoder_input_placeholder = Input(shape = (MAX_HIN_LEN,))
decoder_embedding = Embedding(HINDI_VOCAB_SIZE,300, weights = [hindi_embedding_matrix])
decoder_LSTM = LSTM(200, return_state= True)
decoder_dense1  = Dense(400, activation = 'relu')
decoder_dense2 = Dense(HINDI_VOCAB_SIZE, activation = 'softmax')

decoder_input = decoder_embedding(decoder_input_placeholder)

attention_conc = Concatenate(axis = -1)
attention_repeat = RepeatVector(MAX_HIN_LEN)
attention_dot = Dot(axes = 1)
attention_dense1 = Dense(50, activation = 'tanh')
attention_dense2 = Dense(1, activation = 'sigmoid')

def attention_step(encoder_output,s):
    """
    This funtion performs one step of attention using the conventional approach
    by using a neural net with unputs St-1 and Ht' to calculate context
    
    Args:
    encoder_output : shape = N X Tx X 400 - this is the output of encoder BiLSTM
    s : This is the hidden unit of the previous LSTM decoder cell
    
    Returns:
    context: shape = N X 1 X 400
    
    """
    # This steps repeats the s layer Tx times
    # shape- M  ---> Tx, M
    s_t = attention_repeat(s)
    
    #Concatenates the encoder_output with s_t
    #shape = Tx, M + 400 , where M is the hidden units of decoder LSTM
    attention_input = attention_conc([encoder_output, s_t])
    
    x = attention_dense1(attention_input)
    x = Dropout(0.2)(x)
    alpha = attention_dense2(x)  # These alphas are the weights for encoder, shape = Tx, 1,
    
    context = attention_dot([alpha, encoder_output])
    # context vector to be passed as input to the decoder
    
    return context

concat_teacher_force = Concatenate(axis = 2)
initial_s = Input(shape=(200,))
initial_c = Input(shape=(200,))

s = initial_s
c = initial_c

outputs_ = [] # Collected output list
for t in range(MAX_HIN_LEN):
    
    # get context vector
    context = attention_step(encoder_output,s)
    
    # The select funtion selects the appropriate input
    # word to enable teacher forcing
    select = Lambda(lambda x:x[: , t : t+1])
    x = select(decoder_input)
    
    # Teacher forcing by concatenating context and prev input
    x = concat_teacher_force([context, x])
    
    x, s, c  = decoder_LSTM(x, initial_state = [s,c])
    
    x = decoder_dense1(x)

    #x = Dropout(0.2)(x)
    
    output = decoder_dense2(x)
    
    outputs_.append(output)

#outputs = tf.convert_to_tensor(outputs_)

def change_dim(x):
    """
    This function changes the shape of the outputs list
    It changes the shape from Tx x N x output_vocab_size ----> N x Tx X output_vocab_size
    """
    
    x = K.stack(x) # shape - batch_size x Tx x output_vocan_size
    x = K.permute_dimensions(x, pattern=(1,0,2)) # is now batch_size x T x output_vocab_size
    return x

changer = Lambda(change_dim)
outputs = changer((outputs_))

model = Model(inputs = [encoder_input, decoder_input_placeholder, initial_s, initial_c], outputs = outputs)

model.summary()

plot_model(model)

"""Categorical cross entropy"""

model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])

z = np.zeros((len(input_data),200))

# using one hot encoding and a batch size for fitting model
""" Batch wise one hot encoding to prevent memory error"""

batches = len(input_data)//1000

for i in range(batches):
  #print(i)

  decoder_targets = np.zeros(
    (
      1000,
      20,
      42000
    ),
    dtype='float32'
  )

# assign the values
  for i, d in enumerate(target_tokens_output_pad[i*1000:(i+1)*1000]):
    for t, word in enumerate(d):
      if word > 0:
        decoder_targets[i, t, word] = 1


  z = np.zeros(((1000),200))
  r = model.fit([input_tokens_pad[i*1000 : (i+1)*1000],target_tokens_input_pad[i*1000 : (i+1)*1000], z, z], decoder_targets,verbose = 0, batch_size = 256, epochs = 10)
  if i%5 == 0:
    pred = model.predict([input_tokens_pad[i*1000:(i+1)*1000],target_tokens_input_pad[i*1000:(i+1)*1000], z, z])
    print("Training_accuracy for batch {} : {}".format(i,accuracy_score(np.argmax(decoder_targets), np.argmax(pred))))

"""Sparse Categorical Cross Entropy"""



# CALLBACKS Used in the model 

tb = TensorBoard(write_graph = True, 
                 write_images= False, 
                 histogram_freq= 1)

early_stopper = EarlyStopping(monitor = 'val_accuracy', 
                              patience= 4, 
                              mode = 'max', min_delta = 0.001, 
                              verbose = 1)

# checkpoint = ModelCheckpoint(filepath = filepath,
#                              monitor = 'val_accuracy',
#                              save_best_only= True, 
#                              verbose = 1)

lr_optimizer = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3,
                                 patience = 4, cooldown= 1,
                                 min_lr = 10e-7)

input_tokens_pad.shape



model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])

target_tokens_output_pad_ = target_tokens_output_pad.reshape((*target_tokens_output_pad.shape),1)

z = np.zeros((len(input_data),200))

r = model.fit([input_tokens_pad,target_tokens_input_pad, z, z], target_tokens_output_pad_,
              
              batch_size = 512, epochs = 100,
              validation_split = 0.1)
              #callbacks = [tb, early_stopper, 
                                                   #lr_optimizer])

!tensorboard --logdir=logs

